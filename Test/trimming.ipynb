{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/21 17:18:40 WARN Utils: Your hostname, sarbajit-laptop resolves to a loopback address: 127.0.1.1; using 192.168.0.130 instead (on interface wlp2s0)\n",
      "22/02/21 17:18:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/21 17:18:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NormalProcessor\").getOrCreate()\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+\n",
      "|    Name|Age|Exp|\n",
      "+--------+---+---+\n",
      "|   Ajith| 22|  2|\n",
      "|Jyothish| 21|  3|\n",
      "|  Anandu| 20|  5|\n",
      "|    null| 18|  1|\n",
      "+--------+---+---+\n",
      "\n",
      "+--------+---+---+\n",
      "|    Name|Age|Exp|\n",
      "+--------+---+---+\n",
      "|   Ajith| 22|  2|\n",
      "|Jyothish| 21|  3|\n",
      "|  Anandu| 20|  5|\n",
      "|    null| 18|  1|\n",
      "+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "df = spark.read.format('csv').options(delimiter=',', header=True).load('/home/sarbajit/Documents/Programs/spark/Data/imp/Book1.csv')\n",
    "df.show()\n",
    "df = df.dropna(how=\"any\",subset=['Exp'])\n",
    "df.show()\n",
    "\n",
    "# d = [\n",
    "#     ('','123'),\n",
    "#     ('23','234')\n",
    "# ]\n",
    "\n",
    "# df = spark.createDataFrame(d, schema=['a','b'])\n",
    "# df.show()\n",
    "# df = df.replace(\"\",None)\n",
    "# df.show()\n",
    "\n",
    "\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp = spark.createDataFrame([],schema=['Name','Age','Exp'])\n",
    "\n",
    "# emp=emp.union(df)\n",
    "# emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'fname' : {\n",
    "        'datatype':'String',\n",
    "        'trim' : 'Y'\n",
    "    },\n",
    "\n",
    "    'lname' : {\n",
    "        'datatype':'String',\n",
    "        'trim' : 'Y'\n",
    "    },\n",
    "\n",
    "    'id' : {\n",
    "        'datatype':'Decimal',\n",
    "        'trim' : 'Y',\n",
    "        'scale' : 2,\n",
    "        'precision' : 10\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types\n",
    "def schemaParser():\n",
    "    global schema\n",
    "    needTypeCast=dict()\n",
    "    allFields = []\n",
    "    demoSchema = StructType()\n",
    "    fieldSchema = StructType()\n",
    "    trimFlag = []\n",
    "    for key in schema.keys():\n",
    "        allFields.append(key.title())\n",
    "        if schema[key]['datatype']=='Decimal':\n",
    "            fieldType = getattr(pyspark.sql.types, (schema[key]['datatype'] + 'Type'))(int(schema[key]['precision']), int(schema[key]['scale']))\n",
    "        else:\n",
    "            fieldType = getattr(pyspark.sql.types, (schema[key]['datatype'] + 'Type'))()\n",
    "        fieldSchema.add(key.title(), fieldType)\n",
    "        demoSchema.add(key.title(), StringType())\n",
    "        if schema[key]['datatype']!='String':\n",
    "            needTypeCast[key.title()]=fieldType\n",
    "\n",
    "        if (schema[key]['trim']=='Y'):\n",
    "            trimFlag.append(key.title())\n",
    "\n",
    "    return fieldSchema,demoSchema ,trimFlag, needTypeCast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = \"../Data/imp/test.csv\"\n",
    "\n",
    "fieldSchema, allField, trimFlag, needTypeCast = schemaParser()\n",
    "print(needTypeCast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "parse = udf(lambda record : go(record, ','), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read as single col\n",
    "df = spark.read.format('text').option('delimiter','').load(file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('value', parse(col('value')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('delimiter',',').schema(allField).load(file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in trimFlag:\n",
    "    df = df.withColumn(column, trim(col(column)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in needTypeCast.keys():\n",
    "    df = df.withColumn(column, col(column).cast(needTypeCast[column]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import os\n",
    "file_path = \"../Data/imp/test.csv\"\n",
    "\n",
    "with open(file_path) as csv_file:\n",
    "    rd = csv.reader(csv_file, delimiter=\", \")\n",
    "    for row in rd:\n",
    "        print(\",\".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(record, delimiter):\n",
    "    record = record+','\n",
    "    quoteIndexes = [ i for i, val in enumerate(record) if val=='\"']\n",
    "    if (len(quoteIndexes)&1):\n",
    "        print(\"\\\" are present in data\")\n",
    "    \n",
    "    print(quoteIndexes)\n",
    "    i = 0\n",
    "    j = len(record)\n",
    "    print(\"j\",j)\n",
    "    cols = []\n",
    "    # rec = record\n",
    "    colIndexes= []\n",
    "    count=0\n",
    "    while (i<j):\n",
    "        # print(\"org record->\", record)\n",
    "        # print(\"record    ->\", rec)\n",
    "        start =i\n",
    "        # print(\"strt:\", start)\n",
    "        if record[i]==' ':\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "        if i in quoteIndexes:\n",
    "            quoteIndexes.pop(0)\n",
    "            end = quoteIndexes.pop(0) + 1\n",
    "        else:\n",
    "            # print(record[start:j+1])\n",
    "            end = record[start:j+1].index(delimiter) + start\n",
    "            if '\"' in record[start:end]:\n",
    "                quoteIndexes.pop(0)\n",
    "                quoteIndexes.pop(0)\n",
    "\n",
    "\n",
    "        # print((start, end)) \n",
    "        # colIndexes.append((start,end))\n",
    "        cols.append(record[start:end])\n",
    "        # print(\"value of i\", i)\n",
    "        i=end+1\n",
    "        # count+=1\n",
    "    \n",
    "    # print(colIndexes)\n",
    "    return delimiter.join(cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def go(record, delimiter):\n",
    "    record = record+delimiter\n",
    "    # quoteIndexes = [ i for i, val in enumerate(record) if val=='\"']\n",
    "    quoteCount = 0\n",
    "    quoteEndIndx = -1\n",
    "    colEndIndexes=[]\n",
    "    rec_len = len(record)\n",
    "    for i, val in enumerate(record):\n",
    "        if val=='\"':\n",
    "            if (quoteCount&1 and i<rec_len-1 and record[i+1]==delimiter) or (not quoteCount&1):\n",
    "                quoteCount+=1\n",
    "             \n",
    "        if val==delimiter and not quoteCount&1:\n",
    "            colEndIndexes.append(i)\n",
    "    # print(colEndIndexes)\n",
    "    start = 0\n",
    "    indexLists = []\n",
    "    for i in colEndIndexes:\n",
    "        indexLists.append((start,i))\n",
    "        start=i+1\n",
    "\n",
    "\n",
    "    \n",
    "    return delimiter.join([record[tup[0]:tup[1]].strip() for tup in indexLists])\n",
    "    # return colEndIndexes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_sample = ['123, ABCDES,,, \"Sarbajit \"abd\"s\", Nandy\", ABC, \" 221B, Baker street\"', \n",
    "               '123, \"Sarbajit, Nandy\", ABC,\"221B, Baker street\"', \n",
    "               '123,\"Sarbajit, Nandy\",ABC, \"221B, Baker street\"']\n",
    "[go(data,\",\") for data in data_sample]\n",
    "# 123, ABCDES,,,=\"SarbajitNandy\", ABC, \" 221B, Baker street\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import add_months, col, date_format, from_unixtime,unix_timestamp\n",
    "\n",
    "date = '20210722'\n",
    "s = [(date,)]\n",
    "print(s)\n",
    "df= spark.createDataFrame(s, schema=['value'])\n",
    "df.show()\n",
    "dfO = df.withColumn('value',date_format(add_months(from_unixtime(unix_timestamp(col('value'),'yyyyMMdd')),-12),'yyyyMMdd'))\n",
    "dfO.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|client|    date|\n",
      "+------+--------+\n",
      "|    12|20210506|\n",
      "|    11|    null|\n",
      "|    12|20201206|\n",
      "|    14|20210131|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['12','20210506'],['11', None,], ['12','20201206'], ['14','20210131']], schema=['client','date'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|client|    date|   newDate|\n",
      "+------+--------+----------+\n",
      "|    12|20210506|2021-05-06|\n",
      "|    11|    null|      null|\n",
      "|    12|20201206|2020-12-06|\n",
      "|    14|20210131|2021-01-31|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp,date_format,from_unixtime, col, to_date\n",
    "df.createOrReplaceTempView(\"DF\")\n",
    "\n",
    "# spark.sql(\"selct date_format(from_unixtime(unix_timestamp(col('date'),'yyyyMMdd')),'yyyy-MM-dd') from DF\").show()\n",
    "# df = df.withColumn('newDate',date_format(from_unixtime(unix_timestamp(col('date'),'yyyyMMdd')),'yyyy-MM-dd'))\n",
    "df = df.withColumn('newDate',to_date(col('date'),'yyyyMMdd'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|client|max(newDate)|\n",
      "+------+------------+\n",
      "|    11|        null|\n",
      "|    12|  2021-05-06|\n",
      "|    14|  2021-01-31|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"DF\")\n",
    "spark.sql(\"select client,max(newDate) from DF group by client\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType,DateType,TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(name2,StringType,true),StructField(name3,StringType,true)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType()\n",
    "schema.add('name', StringType(), True)\n",
    "schema.add('name2', StringType(), True)\n",
    "schema.add('name3', StringType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|    Name|       add|LName|\n",
      "+--------+----------+-----+\n",
      "|sarbajit|123,\n",
      "Baker|nandy|\n",
      "|sarbajit|       234|nandy|\n",
      "|     sha|        34|  eno|\n",
      "+--------+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",True)\\\n",
    "    .option('sep',\",\")\\\n",
    "    .option(\"multiLine\", True)\\\n",
    "    .load('/home/sarbajit/Documents/Programs/spark/Data/test.csv')\n",
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "def go(value):\n",
    "    return str(value).replace(\"\\n\",\"\")\n",
    "\n",
    "feed = udf(go, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----+\n",
      "|    Name|      add|LName|\n",
      "+--------+---------+-----+\n",
      "|sarbajit|123,Baker|nandy|\n",
      "|sarbajit|      234|nandy|\n",
      "|     sha|       34|  eno|\n",
      "+--------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for each in df.columns:\n",
    "    df = df.withColumn(each, feed(col(each)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").option(\"path\",\"df.csv\").option(\"delimiter\", ',').option(\"escape\", \"\\n\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|            name2|name3|\n",
      "+-----------------+-----+\n",
      "|asdasdasd,\n",
      "werere|Nandy|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"DF\")\n",
    "\n",
    "spark.sql(\"select name2,name3  from DF\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"name\":\"sarbajit...|\n",
      "|{\"name\":\"Shalmoli...|\n",
      "|{\"name\":\"Supriya\"...|\n",
      "|{\"name\":\"Shuvanka...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.read.format('text').load(\"/home/sarbajit/Documents/Programs/spark/Data/json.json\")\n",
    "rdd1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----+\n",
      "|   c|     name|dept|\n",
      "+----+---------+----+\n",
      "|true| sarbajit| CSE|\n",
      "|null| Shalmoli| CSE|\n",
      "|null|  Supriya|  EE|\n",
      "|null|Shuvankar|  IT|\n",
      "+----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col,lit, from_json\n",
    "from pyspark.sql.types import StructType,StringType\n",
    "import json\n",
    "\n",
    "# JsonUdf = udf(lambda value : json.loads(value), StructType())\n",
    "# for each in ['name','dept','c']:\n",
    "#     rdd1 = rdd1.withColumn(each, lit(None).cast(StringType()))\n",
    "# rdd1.show()\n",
    "schema = StructType()\n",
    "schema.add('c',StringType())\n",
    "schema.add('name',StringType())\n",
    "schema.add('dept',StringType())\n",
    "rdd1.withColumn('value', from_json(col('value'), schema)).select(col('value.*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "|    c|dept|     name|\n",
      "+-----+----+---------+\n",
      "| true| CSE| sarbajit|\n",
      "| null| CSE| Shalmoli|\n",
      "| null|  EE| sarbajit|\n",
      "|false|  IT|Shuvankar|\n",
      "+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df = spark.read.format('json').load(\"/home/sarbajit/Documents/Programs/spark/Data/json.dat\")\n",
    "df.show()\n",
    "\n",
    "cdf = df.select(countDistinct('name').alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.take(1)[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ad1382d28f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrdd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrdd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import json \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "rdd2 = df.rdd.map(lambda row : json.loads(row.value))\n",
    "rdd2.collect()\n",
    "rdd2 = rdd2.map(lambda value : Row(**value))\n",
    "rdd2.collect()\n",
    "# l= rdd2.collect()[0]\n",
    "# print(l.keys())\n",
    "schema = StructType()\n",
    "schema.add('c',StringType(), True)\n",
    "schema.add('name',StringType(), True)\n",
    "schema.add('dept',StringType(), True)\n",
    "df = spark.createDataFrame(rdd2, schema=schema)\n",
    "df.show()\n",
    "# rdd2.toDF().show()\n",
    "# rdd2.saveAsTextFile(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Alic', 'dept': 'cse'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'name': 'Alic', 'dept': 'cse'}\n",
    "s = Row(**a)\n",
    "s.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([['04/12/2021 00:00:00'],['04/11/2021 12:00:00']], schema=['value'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    DATE|\n",
      "+--------+\n",
      "|20210412|\n",
      "|20210411|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, substring,date_format\n",
    "\n",
    "df.createOrReplaceTempView(\"DF\")\n",
    "\n",
    "spark.sql(\"select date_format(to_date(from_unixtime(unix_timestamp(substring(value,1,10),'MM/dd/yyyy'))),'yyyyMMdd') as DATE from DF\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+\n",
      "|    Name|Age|Exp|\n",
      "+--------+---+---+\n",
      "|   Ajith| 22|  2|\n",
      "|Jyothish| 21|  3|\n",
      "|  Anandu| 20|  5|\n",
      "|        | 18|  1|\n",
      "+--------+---+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Exp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF\n",
    "from pyspark.sql.types import StructType,StringType,IntegerType,StructField\n",
    "schema = StructType([\n",
    "    StructField('Name', StringType()),\n",
    "    StructField('Age', StringType()),\n",
    "    StructField('Exp', StringType())\n",
    "])\n",
    "df = spark.read.format('csv').options(delimiter=',', header=True).load('/home/sarbajit/Documents/Programs/spark/Data/imp/Book1.csv', schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`Age`' given input columns: [Department, Manager, Name, Salary];;\n'Project [Name#61, Department#62, Manager#63, Salary#64, contructNames(array(Name#61, 'Age, 'Exp)) AS new#365]\n+- Relation[Name#61,Department#62,Manager#63,Salary#64] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18416/952897248.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Exp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# df.select(ConstructNameUDF(arg)).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstructNameUDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Exp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PASS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \"\"\"\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`Age`' given input columns: [Department, Manager, Name, Salary];;\n'Project [Name#61, Department#62, Manager#63, Salary#64, contructNames(array(Name#61, 'Age, 'Exp)) AS new#365]\n+- Relation[Name#61,Department#62,Manager#63,Salary#64] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, array , when, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# df.select (array(\"Name\",\"Age\").alias('ARR')).show()\n",
    "def contructNames(names) : \n",
    "    if not all([ type(each)==str for each in names]):\n",
    "        raise ValueError(\"List contains non-string element\")\n",
    "    else:\n",
    "        return \" \".join([name.strip() for name in names ])\n",
    "\n",
    "ConstructNameUDF = udf(contructNames, StringType())\n",
    "\n",
    "arg = array(\"Name\",\"Age\",\"Exp\")\n",
    "# df.select(ConstructNameUDF(arg)).show()\n",
    "df=df.withColumn(\"new\", ConstructNameUDF(arg))\n",
    "\n",
    "arg = array(\"Name\",\"Age\",\"Exp\", lit('PASS'))\n",
    "df =df.withColumn(\"new2\", when((col('Name').isNull()) | (col('Name')==' '), col('new')).otherwise(ConstructNameUDF(arg)))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|Exp|Age|\n",
      "+---+---+\n",
      "|  1| 10|\n",
      "|  2| 20|\n",
      "|  3| 30|\n",
      "|  4| 40|\n",
      "|  5| 50|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame([[1,10],[2,20],[3,30],[4,40],[5,50]], schema=['Exp','Age'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+------+\n",
      "|         Name|Department|    Manager|Salary|\n",
      "+-------------+----------+-----------+------+\n",
      "|   Robin Hood|      null|       null|   200|\n",
      "|Arsene Wenger|       Bar| Friar Tuck|    50|\n",
      "|   Friar Tuck|       Foo| Robin Hood|   100|\n",
      "|  Little John|       Foo| Robin Hood|   100|\n",
      "|Sam Allardyce|      null|       null|   250|\n",
      "|Dimi Berbatov|       Foo|Little John|    50|\n",
      "+-------------+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('delimiter',',').option('header',True).load('/home/sarbajit/Documents/Programs/spark/Data/imp/emp.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|         Name|Salary|\n",
      "+-------------+------+\n",
      "|   Robin Hood|   200|\n",
      "|Arsene Wenger|    50|\n",
      "|   Friar Tuck|   100|\n",
      "|  Little John|   100|\n",
      "|Sam Allardyce|   250|\n",
      "|Dimi Berbatov|    50|\n",
      "+-------------+------+\n",
      "\n",
      "+-------------+----------+-----------+------+-------------+\n",
      "|         Name|Department|    Manager|Salary|ManagerSalary|\n",
      "+-------------+----------+-----------+------+-------------+\n",
      "|   Robin Hood|      null|       null|   200|         null|\n",
      "|Arsene Wenger|       Bar| Friar Tuck|    50|          100|\n",
      "|   Friar Tuck|       Foo| Robin Hood|   100|          200|\n",
      "|  Little John|       Foo| Robin Hood|   100|          200|\n",
      "|Sam Allardyce|      null|       null|   250|         null|\n",
      "|Dimi Berbatov|       Foo|Little John|    50|          100|\n",
      "+-------------+----------+-----------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfM = df.select(\"Name\",\"Salary\")\n",
    "dfM.show()\n",
    "cond = \"ManagerSalary\"\n",
    "df.alias('A').join(dfM.alias('B'),how='left',on=[col(\"A.Manager\")==col(\"B.Name\")]).select(\"A.*\", col('B.Salary').alias(cond)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|mp_id|             actions|\n",
      "+-----+--------------------+\n",
      "|  id1|[Open, Close, Unk...|\n",
      "|  id2|    [Close, Unknown]|\n",
      "|  id3|                null|\n",
      "|  id4|           [Unknown]|\n",
      "+-----+--------------------+\n",
      "\n",
      "+-----+-------+\n",
      "|mp_id| action|\n",
      "+-----+-------+\n",
      "|  id1|   Open|\n",
      "|  id2|Unknown|\n",
      "|  id3|   Open|\n",
      "|  id4|   Open|\n",
      "|  id4|Unknown|\n",
      "|  id5|   Open|\n",
      "|  id6|   null|\n",
      "+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+-------+\n",
      "|mp_id|             actions|mp_id| action|\n",
      "+-----+--------------------+-----+-------+\n",
      "|  id3|                null|  id3|   Open|\n",
      "|  id1|[Open, Close, Unk...|  id1|   Open|\n",
      "|  id2|    [Close, Unknown]|  id2|Unknown|\n",
      "|  id4|           [Unknown]|  id4|Unknown|\n",
      "+-----+--------------------+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "s = [\n",
    "    ['id1', ['Open','Close','Unknown']],\n",
    "    ['id2', ['Close','Unknown']],\n",
    "    ['id3', None],\n",
    "    ['id4', ['Unknown']]\n",
    "]\n",
    "\n",
    "s2 = [\n",
    "    ['id1', 'Open'],\n",
    "    ['id2', 'Unknown'],\n",
    "    ['id3', 'Open'],\n",
    "    ['id4', 'Open'],\n",
    "    ['id4', 'Unknown'],\n",
    "    ['id5', 'Open'],\n",
    "    ['id6', None]\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(s, schema=['mp_id', 'actions'])\n",
    "df2 = spark.createDataFrame(s2, schema=['mp_id', 'action'])\n",
    "df.show()\n",
    "df2.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"DF\")\n",
    "df2.createOrReplaceTempView(\"DF2\")\n",
    "\n",
    "dsql  = \"SELECT * from DF a inner join DF2 b on a.mp_id=b.mp_id and ( a.actions is null or array_contains(a.actions, b.action))\"\n",
    "spark.sql(dsql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Sarbajit'),\n",
       " Row(id=2, name='Shalmoli'),\n",
       " Row(id=3, name='Supriya'),\n",
       " Row(id=4, name='Shuvo')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext\n",
    "rdd1 = spark.createDataFrame([\n",
    "        (1,\"Sarbajit\"),\n",
    "        (2,\"Shalmoli\"),\n",
    "        (3,\"Supriya\"),\n",
    "        (4,\"Shuvo\")\n",
    "    ], schema=['id','name']).rdd\n",
    "\n",
    "rdd1.collect()\n",
    "rdd2 = spark.createDataFrame([\n",
    "        (1,\"CSE\"),\n",
    "        (2,\"IT\"),\n",
    "        (3,\"CSE\"),\n",
    "        (4,None)\n",
    "    ], schema=['id','Dept']).rdd\n",
    "\n",
    "rdd2.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|client|min(ind)|\n",
      "+------+--------+\n",
      "|    c1|       0|\n",
      "|    c4|       0|\n",
      "|    c3|       1|\n",
      "|    c2|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "s = [\n",
    "    [\"c1\",1],\n",
    "    [\"c2\",1],\n",
    "    [\"c3\",1],\n",
    "    [\"c1\",0],\n",
    "    [\"c4\",0],\n",
    "    [\"c1\",1],\n",
    "    [\"c3\",1]\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(s, schema=['client','ind'])\n",
    "# df.show()\n",
    "# df.createOrReplaceTempView(\"MAIN\")\n",
    "\n",
    "# sql = \"select client,\\\n",
    "#      case when min(ind)=0 then 0 else 1 end Indicator\\\n",
    "#      from MAIN group by client\"\n",
    "# spark.sql(sql).show()\n",
    "\n",
    "df.groupBy(\"client\").min(\"ind\").alias(\"indicator\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+\n",
      "| id|     name|roll|\n",
      "+---+---------+----+\n",
      "|  1| sarbajit|  19|\n",
      "|  2| shalmoli|  20|\n",
      "|  3|  supriya|  21|\n",
      "|  4|shuvankar|  32|\n",
      "+---+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = (\n",
    "    (1,'sarbajit',19),\n",
    "    (2,'shalmoli',20),\n",
    "    (3,'supriya',21),\n",
    "    (4,'shuvankar',32),\n",
    ")\n",
    "\n",
    "spark.createDataFrame(data,schema=['id','name','roll']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=============================================>          (61 + 4) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|NEW_ID|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     2|\n",
      "|     3|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import IntegerType\n",
    "d = [\n",
    "     [1,'sarbajit'],\n",
    "     [2,'Shal'],\n",
    "     [3,'Sup'],\n",
    "     [4,'Shu']\n",
    "]\n",
    "df = spark.createDataFrame(d, schema=['id','name'])\n",
    "df.createOrReplaceTempView(\"DF\")\n",
    "df1 = spark.createDataFrame(d, schema=['id','name'])\n",
    "df1.createOrReplaceTempView(\"DF1\")\n",
    "# spark.sql('select max(X.id) from DF X inner join DF Y on X.id<Y.id and Y.id=3').show()\n",
    "sql = \"SELECT CASE WHEN A.id in (3,4) then (select max(X.id) from DF X, DF Y where X.id<Y.id and Y.id=A.id) ELSE A.id END as NEW_ID from DF A\"\n",
    "spark.sql(sql).show()\n",
    "# def D3(id):\n",
    "#     global spark\n",
    "#     print(id)\n",
    "#     sql = f\"select max(id) id from DF where id<{id}\"\n",
    "#     df= spark.sql(sql)\n",
    "#     # df.show()\n",
    "#     print(df.collect()[0]['id'])\n",
    "#     return 12\n",
    "\n",
    "# FUNC = udf(D3, IntegerType())\n",
    "# df = df.withColumn(\"expected_batch_date\", FUNC(col('id')))\n",
    "# df.show()\n",
    "# print(D3(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7acf4c38bcf2513b8cb734d877e501929c7062eca960ed16fb7ee116ea8ae2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
