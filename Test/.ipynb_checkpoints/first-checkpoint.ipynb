{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e91d34768ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-28f9c69b18a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(master=\"local\", appName=\"test\")\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_name=\"/home/sarbajit/Documents/Programs/spark/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, FloatType, StringType \n",
    "iris_schema = StructType([\n",
    "    StructField(\"sepal_length\", FloatType()),\n",
    "    StructField(\"sepal_width\", FloatType()),\n",
    "    StructField(\"petal_length\", FloatType()),\n",
    "    StructField(\"petal_width\", FloatType()),\n",
    "    StructField('species', StringType())\n",
    "])\n",
    "iris = spark.read.csv(path=os.path.join(dir_name, \"iris.csv\"), schema=iris_schema)\n",
    "\n",
    "iris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|sepal_length|petal_length|\n",
      "+------------+------------+\n",
      "|         5.1|         1.4|\n",
      "|         4.9|         1.4|\n",
      "|         4.7|         1.3|\n",
      "|         4.6|         1.5|\n",
      "|         5.0|         1.4|\n",
      "|         5.4|         1.7|\n",
      "|         4.6|         1.4|\n",
      "|         5.0|         1.5|\n",
      "|         4.4|         1.4|\n",
      "|         4.9|         1.5|\n",
      "|         5.4|         1.5|\n",
      "|         4.8|         1.6|\n",
      "|         4.8|         1.4|\n",
      "|         4.3|         1.1|\n",
      "|         5.8|         1.2|\n",
      "|         5.7|         1.5|\n",
      "|         5.4|         1.3|\n",
      "|         5.1|         1.4|\n",
      "|         5.7|         1.7|\n",
      "|         5.1|         1.5|\n",
      "+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.createOrReplaceTempView(\"iris_temp_table\")\n",
    "sqlContext.sql(\"select sepal_length, petal_length from iris_temp_table\").show()\n",
    "# sqlContext.dropTempTable(\"iris_temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "|      db|           iris|      false|\n",
      "|        |iris_temp_table|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlContext.tableNames()\n",
    "sqlContext.tables().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5.1', '1.4'),\n",
       " ('4.9', '1.4'),\n",
       " ('4.7', '1.3'),\n",
       " ('4.6', '1.5'),\n",
       " ('5', '1.4'),\n",
       " ('5.4', '1.7'),\n",
       " ('4.6', '1.4'),\n",
       " ('5', '1.5'),\n",
       " ('4.4', '1.4'),\n",
       " ('4.9', '1.5')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_rdd = sc.textFile(os.path.join(dir_name, \"iris.csv\"))\n",
    "\n",
    "iris_rdd_split = iris_rdd.map(lambda line : line.split(\",\")).map(lambda col : (col[0], col[2]))\n",
    "iris_rdd_split.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| _1|             _2|\n",
      "+---+---------------+\n",
      "|  a|   [10, 20, 30]|\n",
      "|  b|[100, 200, 300]|\n",
      "|  m|           [,,]|\n",
      "+---+---------------+\n",
      "\n",
      "http://192.168.0.16:4040\n"
     ]
    }
   ],
   "source": [
    "# python object to rdd\n",
    "objToRdd =sc.parallelize([\n",
    "    ('a', [10,20,30]), ('b',[100,200,300]), ('m', ['a','b','m'])\n",
    "])\n",
    "value\n",
    "# print(objToRdd.collect())\n",
    "df = spark.createDataFrame(objToRdd)\n",
    "df.show()\n",
    "print(sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive introduction\n",
    "sqlContext.sql(\"drop database db\")\n",
    "\n",
    "sqlContext.sql(\"create database db\")\n",
    "sqlContext.sql(\"use db\")\n",
    "\n",
    "from pyspark.sql import DataFrameWriter\n",
    "dfw = DataFrameWriter(iris)\n",
    "# sqlContext.sql(\"drop table iris\")\n",
    "dfw.saveAsTable(name=\"iris\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+-----------+\n",
      "|database|      tableName|isTemporary|\n",
      "+--------+---------------+-----------+\n",
      "|      db|           iris|      false|\n",
      "|        |iris_temp_table|       true|\n",
      "+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.tables().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+------------------+\n",
      "|        species|sum of Sepal length|sum_of_Sepal_width|\n",
      "+---------------+-------------------+------------------+\n",
      "| Iris-virginica|              329.4|             148.7|\n",
      "|    Iris-setosa|              250.3|             170.9|\n",
      "|Iris-versicolor|              296.8|             138.5|\n",
      "+---------------+-------------------+------------------+\n",
      "\n",
      "+-------------------+\n",
      "|sum of Sepal length|\n",
      "+-------------------+\n",
      "|              329.4|\n",
      "|              250.3|\n",
      "|              296.8|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF \n",
    "from pyspark.sql.functions import col, udf, collect_list\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def makeItUpperCase(value):\n",
    "    # print(value)\n",
    "    return value.upper()\n",
    "\n",
    "doUpper = udf(makeItUpperCase, StringType())\n",
    "\n",
    "# iris.select(doUpper(iris.species)).show()\n",
    "# iris.withColumn(\"species\", doUpper(iris.species)).show()\n",
    "# iris.show()\n",
    "\n",
    "df = iris.groupBy(iris.species).agg(collect_list(iris.sepal_length).alias(\"agg_sepal_length\"),collect_list(iris.sepal_width).alias(\"agg_sepal_width\"))\n",
    "# print(type(df))\n",
    "\n",
    "getSum = udf(lambda var : sum(var), returnType=FloatType())\n",
    "\n",
    "agg_iris = df.select(df.species,getSum(df.agg_sepal_length).alias(\"sum of Sepal length\"), getSum(df.agg_sepal_width).alias(\"sum_of_Sepal_width\"))\n",
    "\n",
    "agg_iris.show()\n",
    "agg_iris.select(col(\"sum of Sepal length\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|doUpperCase(species)|\n",
      "+--------------------+\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "|         IRIS-SETOSA|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"doUpperCase\", makeItUpperCase, StringType())\n",
    "\n",
    "sqlContext.sql(\"select doUpperCase(species) from iris\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.0.10:4040'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching and Persisting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./iris.csv MapPartitionsRDD[149] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "# iris.rdd.setName(\"iris\")\n",
    "df = sc.textFile(\"./iris.csv\")\n",
    "print(df)\n",
    "# df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('1', 3), ('11', 1)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sc.parallelize((('1',),('1'),('11',1),('11'))).countByKey().items()\n",
    "# sc.parallelize((('a',1),('b',2),('a',3),('a',4))).foldByKey(0,add).items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-f06fd6452509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msepal_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Column'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "iris.select(int(iris.sepal_length)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|     _c0|       _c1|\n",
      "+--------+----------+\n",
      "|Sarbajit|Na\"n\\\"\\\"dy|\n",
      "| Krishna|     Nandy|\n",
      "+--------+----------+\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.format('csv').option(\"delimiter\", \"|\").option('mode',\"FAILFAST\").load(os.path.join(dir_name,'data.txt'))\n",
    "data.show()\n",
    "print(data.count())\n",
    "data.write.mode(\"Overwrite\").format(\"parquet\").option('path',os.path.join(dir_name,'out.parquet')).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|     _c0|       _c1|\n",
      "+--------+----------+\n",
      "|Sarbajit|Na\"n\\\"\\\"dy|\n",
      "| Krishna|     Nandy|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"parquet\").load(os.path.join(dir_name,'out.parquet')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b61898a79947ac66558edacc18b76b0b8053dee70fb4677026e050cd4867134d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
