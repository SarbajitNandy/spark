{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_xml='/home/sarbajit/Documents/Programs/spark/jars/spark-xml_2.12-0.12.0.jar'\n",
    "xmlschemaPath = '/home/sarbajit/Documents/Programs/spark/jars/XmlSchema-1.4.7.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - pydeequ\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/linux-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please set env variable SPARK_VERSION\n",
      "22/01/27 15:40:37 WARN Utils: Your hostname, sarbajit-laptop resolves to a loopback address: 127.0.1.1; using 192.168.0.130 instead (on interface wlp2s0)\n",
      "22/01/27 15:40:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/sarbajit/.ivy2/cache\n",
      "The jars for the packages stored in: /home/sarbajit/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c30564dd-60be-4bbd-acfb-e59b85882274;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "downloading https://repo1.maven.org/maven2/com/amazon/deequ/deequ/1.2.2-spark-3.0/deequ-1.2.2-spark-3.0.jar ...\n",
      "\t[SUCCESSFUL ] com.amazon.deequ#deequ;1.2.2-spark-3.0!deequ.jar (3242ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze_2.12/0.13.2/breeze_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze_2.12;0.13.2!breeze_2.12.jar (16074ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scalanlp/breeze-macros_2.12/0.13.2/breeze-macros_2.12-0.13.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scalanlp#breeze-macros_2.12;0.13.2!breeze-macros_2.12.jar (251ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (449ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (301ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/rwl/jtransforms/2.4.0/jtransforms-2.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.rwl#jtransforms;2.4.0!jtransforms.jar (664ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.2/commons-math3-3.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.2!commons-math3.jar (1548ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire_2.12/0.13.0/spire_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire_2.12;0.13.0!spire_2.12.jar (7304ms)\n",
      "downloading https://repo1.maven.org/maven2/com/chuusai/shapeless_2.12/2.3.2/shapeless_2.12-2.3.2.jar ...\n",
      "\t[SUCCESSFUL ] com.chuusai#shapeless_2.12;2.3.2!shapeless_2.12.jar(bundle) (3316ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.5!slf4j-api.jar (258ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.1/scala-reflect-2.12.1.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.1!scala-reflect.jar (4196ms)\n",
      "downloading https://repo1.maven.org/maven2/junit/junit/4.8.2/junit-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] junit#junit;4.8.2!junit.jar (1218ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spire-math/spire-macros_2.12/0.13.0/spire-macros_2.12-0.13.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spire-math#spire-macros_2.12;0.13.0!spire-macros_2.12.jar (313ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/machinist_2.12/0.6.1/machinist_2.12-0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#machinist_2.12;0.6.1!machinist_2.12.jar (303ms)\n",
      "downloading https://repo1.maven.org/maven2/org/typelevel/macro-compat_2.12/1.1.1/macro-compat_2.12-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.typelevel#macro-compat_2.12;1.1.1!macro-compat_2.12.jar (202ms)\n",
      ":: resolution report :: resolve 75684ms :: artifacts dl 39672ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   15  |   15  |   1   ||   15  |   15  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c30564dd-60be-4bbd-acfb-e59b85882274\n",
      "\tconfs: [default]\n",
      "\t15 artifacts copied, 0 already retrieved (32953kB/133ms)\n",
      "22/01/27 15:42:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|   c|\n",
      "+---+---+----+\n",
      "|foo|  1|   5|\n",
      "|bar|  2|   6|\n",
      "|baz|  3|null|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "# spark = SparkSession.builder \\\n",
    "#         .appName(\"XMLProcessor\") \\\n",
    "#         .config(\"spark.jars\",'/home/sarbajit/Documents/Programs/spark/jars') \\\n",
    "#         .config(\"spark.executor.extraClassPath\",spark_xml + \":\" + xmlschemaPath) \\\n",
    "#         .config(\"spark.executor.extraLibrary\",spark_xml) \\\n",
    "#         .config(\"spark.driver.extraClassPath\",spark_xml + \":\" + xmlschemaPath) \\\n",
    "#         .getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"XmlProcessor\").getOrCreate()\n",
    "\n",
    "import pydeequ\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.sparkContext.parallelize([\n",
    "            Row(a=\"foo\", b=1, c=5),\n",
    "            Row(a=\"bar\", b=2, c=6),\n",
    "            Row(a=\"baz\", b=3, c=None)]).toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----+-----+\n",
      "| entity|instance|name|value|\n",
      "+-------+--------+----+-----+\n",
      "|Dataset|       *|Size|  3.0|\n",
      "+-------+--------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .run()\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/Book (copy).xml'\n",
    "\n",
    "# df = spark.read.format('xml').options(rowTag='book', attributePrefix='Attr_', ).load(file_path)\n",
    "df = spark.read.format('xml').options(rowTag='catalog', rowValidationXSDPath=\"/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema2.xsd\", inferSchema=True).load(file_path)\n",
    "df.show()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.java_gateway import java_import\n",
    "java_import(spark.sparkContext._gateway.jvm, \"com.databricks.spark.xml.util.XSDToSchema,java.nio.file.Paths\")\n",
    "jvm = spark.sparkContext._gateway.jvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:com.databricks.spark.xml.util.XSDToSchema.read.\n: java.lang.NoSuchMethodError: org.apache.ws.commons.schema.XmlSchemaCollection.read(Ljava/io/Reader;)Lorg/apache/ws/commons/schema/XmlSchema;\n\tat com.databricks.spark.xml.util.XSDToSchema$.read(XSDToSchema.scala:78)\n\tat com.databricks.spark.xml.util.XSDToSchema.read(XSDToSchema.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-24f6475880eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# obj = jvm.java.nio.file.Paths.get('/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabricks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXSDToSchema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:com.databricks.spark.xml.util.XSDToSchema.read.\n: java.lang.NoSuchMethodError: org.apache.ws.commons.schema.XmlSchemaCollection.read(Ljava/io/Reader;)Lorg/apache/ws/commons/schema/XmlSchema;\n\tat com.databricks.spark.xml.util.XSDToSchema$.read(XSDToSchema.scala:78)\n\tat com.databricks.spark.xml.util.XSDToSchema.read(XSDToSchema.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "# obj = jvm.java.nio.file.Paths.get('/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd')\n",
    "schema = jvm.com.databricks.spark.xml.util.XSDToSchema.read('/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd')\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,explode_outer\n",
    "\n",
    "dfout = df.select(col('catalog.*'), col('family.*'), \"*\")\n",
    "dfout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout.show()\n",
    "dfout= dfout.withColumn('book',explode_outer(col('book')))\n",
    "dfout=dfout.withColumn('member',explode_outer(col('member')))\n",
    "# dfout=dfout.select('book','member')\n",
    "dfout.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = dfout.select ('book.*','member.*')\n",
    "dfout.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMap = {\n",
    "    'catalog.book._id':'bookId',\n",
    "    'catalog.book.author' : 'author',\n",
    "    'catalog.book.title' : 'title',\n",
    "    'catalog.book.genre' : 'genre',\n",
    "    'catalog.book.price' : 'price',\n",
    "    'catalog.book.publish_date' : 'publish_date',\n",
    "    'family.member._id': 'member_id',\n",
    "    'family.member.firstname': 'firstname',\n",
    "    'family.member.lastname': 'lastname',\n",
    "    'family.member.age': 'age',\n",
    "    'family.member.profession': 'profession',\n",
    "    'family.member.price': 'price',\n",
    "    'family.member.sex': 'sex',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av=[]\n",
    "\n",
    "for node in dataMap.keys():\n",
    "    try:\n",
    "        df.select(col(node))\n",
    "        av.append(node)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(av)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = []\n",
    "for each in av:\n",
    "    colList.append(col(each).alias(dataMap[each]))\n",
    "\n",
    "df.select(*colList).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmlschema\n",
    "schema_details = xmlschema.XMLSchema('/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd')\n",
    "\n",
    "# print(schema.attribute_form_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "jvm = spark.sparkContext._gateway.jvm\n",
    "java_import(jvm,'com.databricks.spark.xml.util.*')\n",
    "\n",
    "schema = jvm.XSDToSchema('/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd')\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_df=spark.createDataFrame(schema=schema_details, data=[])\n",
    "xml_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in schema.iter_components():\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = schema.decode(\"/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/Book.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/data.json','w') as fp:\n",
    "    json.dump(data, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = schema.complex_types[0]\n",
    "isinstance(node,xmlschema.validators.complex_types.XsdComplexType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(*['author','description']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "rdd1 = sc.textFile('/home/sarbajit/Documents/Programs/spark/Data/imp/Emp.txt')\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmlschema\n",
    "xsdFile = \"/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd\"\n",
    "schema = xmlschema.XMLSchema(xsdFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dir\", 'w') as fp:\n",
    "    fp.write(str(dir(schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlFile= '/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/Book.xml'\n",
    "data = schema.to_dict(xmlFile)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.decode(xmlFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType,StructType, ArrayType,StructField,DateType\n",
    "xsdFile = \"/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/BookXMLSchema.xsd\"\n",
    "xmlFile= '/home/sarbajit/Documents/Programs/spark/Data/xml/BookDetails/Book.xml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- description: struct (nullable = true)\n",
      " |    |-- hold: struct (nullable = true)\n",
      " |    |    |-- holdingDetails: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- a: long (nullable = true)\n",
      " |    |    |    |    |-- b: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- publish_date: date (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('xml').options(rowTag='book',inferSchema=True).load(xmlFile)\n",
    "# df.show()\n",
    "schema2=df.schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+\n",
      "|  _id|   a|   b|\n",
      "+-----+----+----+\n",
      "|bk101| 123| abc|\n",
      "|bk101| 123| abc|\n",
      "|bk102| 123| abc|\n",
      "|bk102| 234|abc1|\n",
      "|bk103|null|null|\n",
      "|bk104|null|null|\n",
      "|bk105|null|null|\n",
      "|bk106|null|null|\n",
      "|bk107|null|null|\n",
      "|bk108|null|null|\n",
      "|bk109|null|null|\n",
      "|bk110|null|null|\n",
      "|bk111|null|null|\n",
      "|bk112|null|null|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_id', explode_outer('description.hold.holdingDetails')).select(\"_id\",'col.*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+-----+------------+-----------+\n",
      "|  _id|              author|               title|          genre|price|publish_date|description|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+-----------+\n",
      "|bk101|Gambardella, Matthew|XML Developer's G...|       Computer|44.95|        null|     {null}|\n",
      "|bk102|          Ralls, Kim|       Midnight Rain|        Fantasy| 5.95|        null|     {null}|\n",
      "|bk103|         Corets, Eva|     Maeve Ascendant|        Fantasy| 5.95|        null|     {null}|\n",
      "|bk104|         Corets, Eva|     Oberon's Legacy|        Fantasy| 5.95|        null|     {null}|\n",
      "|bk105|         Corets, Eva|  The Sundered Grail|        Fantasy| 5.95|        null|     {null}|\n",
      "|bk106|    Randall, Cynthia|         Lover Birds|        Romance| 4.95|        null|     {null}|\n",
      "|bk107|      Thurman, Paula|       Splish Splash|        Romance| 4.95|        null|     {abcd}|\n",
      "|bk108|       Knorr, Stefan|     Creepy Crawlies|         Horror| 4.95|        null|     {null}|\n",
      "|bk109|        Kress, Peter|        Paradox Lost|Science Fiction| 6.95|        null|     {null}|\n",
      "|bk110|        O'Brien, Tim|Microsoft .NET: T...|       Computer|36.95|        null|     {null}|\n",
      "|bk111|        O'Brien, Tim|MSXML3: A Compreh...|       Computer|36.95|        null|     {null}|\n",
      "|bk112|         Galos, Mike|Visual Studio 7: ...|       Computer|49.95|        null|     {null}|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_id,StringType,true),StructField(author,StringType,true),StructField(title,StringType,true),StructField(genre,StringType,true),StructField(price,StringType,true),StructField(publish_date,DateType,true),StructField(description,StructType(List(StructField(_VALUE,StringType,true))),true)))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField('_id', StringType(),True),\n",
    "    StructField('author', StringType(),True),\n",
    "    StructField('title', StringType(),True),\n",
    "    StructField('genre', StringType(),True),\n",
    "    StructField('price', StringType(),True),\n",
    "    StructField('publish_date', DateType(),True),\n",
    "    StructField('description', StructType([\n",
    "        StructField(\"_VALUE\",StringType(),True)\n",
    "    ]),True),\n",
    "])\n",
    "\n",
    "df = spark.read.format('xml').options(rowTag='book').load(xmlFile, schema=schema)\n",
    "df.show()\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '_id',\n",
       " 'author': 'author',\n",
       " 'title': 'title',\n",
       " 'genre': 'genre',\n",
       " 'price': 'price',\n",
       " 'publish_date': 'publish_date',\n",
       " 'description': {'hold': {'holdingDetails': [{'a': 'a', 'b': 'b'}]}}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = {\n",
    "    \"A\":'a1',\n",
    "    \"B.name\":\"bname\",\n",
    "    \"B.dept\":\"bdept\",\n",
    "    \"C.name._id\":\"cname_id\",\n",
    "    \"C.name._VALUE\":\"cname_value\"  ,\n",
    "    \"C.roll._id\":\"croll_id\"  ,\n",
    "    \"C.roll._VALUE\":\"croll_value\"  ,\n",
    "    'D.person.details.age': \"pAge\",  \n",
    "    'D.person.details.sex': \"pSex\",  \n",
    "    'D.attach.details.a1': \"pOth\",  \n",
    "}\n",
    "\n",
    "sample2 = {\n",
    "    \"_id\":\"_id\",\n",
    "    \"author\":\"author\",\n",
    "    \"title\":\"title\",\n",
    "    \"genre\":\"genre\",\n",
    "    \"price\":\"price\",\n",
    "    \"publish_date\":\"publish_date\",\n",
    "    \"description\":{\n",
    "        \"hold\":{\n",
    "            'holdingDetails':[{'a':'a','b':'b'}]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def go(node, r):\n",
    "    next=set()\n",
    "    head=dict()\n",
    "    for k,v in node.items():\n",
    "        if '.' not in k:\n",
    "            head[k] = head.get(k,v)\n",
    "        else:\n",
    "            k1= k.split(\".\")[0]\n",
    "            k2=k.split(k1+'.')[1]\n",
    "            # print(\"keys\",k1,k2)\n",
    "            nH=head.get(k1,dict())\n",
    "            # print(nH)\n",
    "            nH[k2]=v\n",
    "            head[k1]=nH\n",
    "            if '.' in k2:\n",
    "                next.add(k1)\n",
    "        # print(head)\n",
    "    for k in next:\n",
    "        head[k]=go(head[k],r+1)\n",
    "    return head\n",
    "\n",
    "rawSchema=go(sample2,1)\n",
    "rawSchema\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catalog': {'book': [{'author': 'author',\n",
       "    'title': 'title',\n",
       "    'genre': 'genre',\n",
       "    'price': 'price',\n",
       "    'publish_date': 'publish_date',\n",
       "    'description': {'_id': 'descriptionId', '_VALUE': 'descriptionValue'}}]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawSchema = {\n",
    "    \"catalog\":{\"book\":[rawSchema]}\n",
    "    \n",
    "}\n",
    "rawSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'struct',\n",
       " 'fields': [{'name': '_id',\n",
       "   'type': 'string',\n",
       "   'nullable': True,\n",
       "   'metadata': {}},\n",
       "  {'name': 'author', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
       "  {'name': 'title', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
       "  {'name': 'genre', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
       "  {'name': 'price', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
       "  {'name': 'publish_date', 'type': 'string', 'nullable': True, 'metadata': {}},\n",
       "  {'name': 'description',\n",
       "   'type': {'type': 'struct',\n",
       "    'fields': [{'name': 'hold',\n",
       "      'type': {'type': 'struct',\n",
       "       'fields': [{'name': 'holdingDetails',\n",
       "         'type': {'type': 'array',\n",
       "          'elementType': {'type': 'struct',\n",
       "           'fields': [{'name': 'a',\n",
       "             'type': 'string',\n",
       "             'nullable': True,\n",
       "             'metadata': {}},\n",
       "            {'name': 'b',\n",
       "             'type': 'string',\n",
       "             'nullable': True,\n",
       "             'metadata': {}}]},\n",
       "          'containsNull': True},\n",
       "         'nullable': True,\n",
       "         'metadata': {}}]},\n",
       "      'nullable': True,\n",
       "      'metadata': {}}]},\n",
       "   'nullable': True,\n",
       "   'metadata': {}}]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def build(node):\n",
    "    baseSchema = None\n",
    "    if type(node)==dict:\n",
    "        inside= []\n",
    "        for k,v in node.items():\n",
    "            if type(v)==dict:\n",
    "                t= build(v)\n",
    "            elif type(v)==list:\n",
    "                pass\n",
    "            else:\n",
    "                t= StringType()\n",
    "            inside.append(StructField(k,t,True))\n",
    "        baseSchema=StructType(inside)\n",
    "        return baseSchema\n",
    "\n",
    "    return None        \n",
    "        \n",
    "def build2(node):\n",
    "    baseSchema = None\n",
    "    if type(node)==dict:\n",
    "        inside= []\n",
    "        for k,v in node.items():\n",
    "            if type(v)==dict:\n",
    "                t= build2(v)\n",
    "            elif type(v)==list:\n",
    "                t = build2(v[0])\n",
    "                t= ArrayType(t, True)\n",
    "            else:\n",
    "                t= StringType()\n",
    "            inside.append(StructField(k,t,True))\n",
    "        baseSchema=StructType(inside)\n",
    "        return baseSchema\n",
    "    elif type(node)==list:\n",
    "        t = build2(node[0])\n",
    "        return ArrayType(t, True)\n",
    "    else:\n",
    "        return StringType()\n",
    "\n",
    "    return None        \n",
    "schema = build2(rawSchema)\n",
    "schema.jsonValue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|  _id|              author|               title|          genre|price|publish_date|         description|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "|bk101|Gambardella, Matthew|XML Developer's G...|       Computer|44.95|  2000-10-01|{{[{123, abc}, {1...|\n",
      "|bk102|          Ralls, Kim|       Midnight Rain|        Fantasy| 5.95|  2000-12-16|{{[{123, abc}, {2...|\n",
      "|bk103|         Corets, Eva|     Maeve Ascendant|        Fantasy| 5.95|  2000-11-17|              {null}|\n",
      "|bk104|         Corets, Eva|     Oberon's Legacy|        Fantasy| 5.95|  2001-03-10|              {null}|\n",
      "|bk105|         Corets, Eva|  The Sundered Grail|        Fantasy| 5.95|        null|              {null}|\n",
      "|bk106|    Randall, Cynthia|         Lover Birds|        Romance| 4.95|  2000-09-02|              {null}|\n",
      "|bk107|      Thurman, Paula|       Splish Splash|        Romance| 4.95|  2000-11-02|              {null}|\n",
      "|bk108|       Knorr, Stefan|     Creepy Crawlies|         Horror| 4.95|  2000-12-06|              {null}|\n",
      "|bk109|        Kress, Peter|        Paradox Lost|Science Fiction| 6.95|  2000-11-02|              {null}|\n",
      "|bk110|        O'Brien, Tim|Microsoft .NET: T...|       Computer|36.95|        null|              {null}|\n",
      "|bk111|        O'Brien, Tim|MSXML3: A Compreh...|       Computer|36.95|        null|              {null}|\n",
      "|bk112|         Galos, Mike|Visual Studio 7: ...|       Computer|49.95|        null|              {null}|\n",
      "+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- publish_date: string (nullable = true)\n",
      " |-- description: struct (nullable = true)\n",
      " |    |-- hold: struct (nullable = true)\n",
      " |    |    |-- holdingDetails: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- a: string (nullable = true)\n",
      " |    |    |    |    |-- b: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('xml').options(rowTag='book').load(xmlFile, schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+\n",
      "|  _id|   a|   b|\n",
      "+-----+----+----+\n",
      "|bk101| 123| abc|\n",
      "|bk101| 123| abc|\n",
      "|bk102| 123| abc|\n",
      "|bk102| 234|abc1|\n",
      "|bk103|null|null|\n",
      "|bk104|null|null|\n",
      "|bk105|null|null|\n",
      "|bk106|null|null|\n",
      "|bk107|null|null|\n",
      "|bk108|null|null|\n",
      "|bk109|null|null|\n",
      "|bk110|null|null|\n",
      "|bk111|null|null|\n",
      "|bk112|null|null|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('_id',explode_outer(\"description.hold.holdingDetails\")).select('_id',\"col.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, explode_outer\n",
    "\n",
    "# df.select(explode_outer(\"catalog.book\").alias(\"book\")).select(col(\"book.*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              _xmlns|          _xmlns:xsi|             catalog|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|http://www.exampl...|http://www.w3.org...|{[{bk101, Gambard...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- _xmlns: string (nullable = true)\n",
      " |-- _xmlns:xsi: string (nullable = true)\n",
      " |-- catalog: struct (nullable = true)\n",
      " |    |-- book: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _id: string (nullable = true)\n",
      " |    |    |    |-- author: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- description2: string (nullable = true)\n",
      " |    |    |    |-- genre: string (nullable = true)\n",
      " |    |    |    |-- price: double (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('xml').options(rowTag='main').load(xmlFile)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|              _xmlns|          _xmlns:xsi|             catalog|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|http://www.exampl...|http://www.w3.org...|{[{bk101, Gambard...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- _xmlns: string (nullable = true)\n",
      " |-- _xmlns:xsi: string (nullable = true)\n",
      " |-- catalog: struct (nullable = true)\n",
      " |    |-- book: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _id: string (nullable = true)\n",
      " |    |    |    |-- author: string (nullable = true)\n",
      " |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |-- description2: string (nullable = true)\n",
      " |    |    |    |-- genre: string (nullable = true)\n",
      " |    |    |    |-- price: double (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('xml').options(rowTag='main').load(xmlFile)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = [['123',4,5,6,], ['123',4,5,7,],['234',4,5,6,],['345',4,5,6,] ,['456',4,5,6,] ]\n",
    "p = [['123',4,5,6,], ['123',4,5,7,],['234',4,5,6,],['345',4,5,6,] ,['456',4,5,6,]]\n",
    "dfs = spark.createDataFrame(s,['id','a','b','c'])\n",
    "dfs.printSchema()\n",
    "dfp = spark.createDataFrame(p,['id','a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "+---+\n",
      "\n",
      "+---+---+---+---+\n",
      "| id|  a|  b|  c|\n",
      "+---+---+---+---+\n",
      "+---+---+---+---+\n",
      "\n",
      "+---+---+---+---+\n",
      "| id|  a|  b|  c|\n",
      "+---+---+---+---+\n",
      "|123|  4|  5|  6|\n",
      "|123|  4|  5|  7|\n",
      "|234|  4|  5|  6|\n",
      "|345|  4|  5|  6|\n",
      "|456|  4|  5|  6|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct, col, lit\n",
    "def go(colList, df):\n",
    "    cols = df.columns\n",
    "    for each in colList:\n",
    "        if each not in cols:\n",
    "            df = df.withColumn(each, lit(None))\n",
    "\n",
    "    return df\n",
    "unAll = dfs.select((col('id'))).distinct()\n",
    "# unAll.show()\n",
    "unqSet = dfp.select(col('id')).distinct()\n",
    "# unqSet.show()\n",
    "inter = unAll.subtract(unqSet)\n",
    "inter.show()\n",
    "inter = go(dfp.columns,inter)\n",
    "inter.show()\n",
    "inter.count()\n",
    "# dfp.unionByName(inter, allowMissingColumns=True).show()\n",
    "l = dfp.union(inter)\n",
    "l.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: string (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs.union(dfp).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(a,StringType,true),StructField(x,StructType(List(StructField(y,ArrayType(StringType,true),true))),true)))\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: map (valueContainsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      "\n",
      "+---+--------------------+\n",
      "| _1|                  _2|\n",
      "+---+--------------------+\n",
      "|  1|{x -> {y -> [1, 2...|\n",
      "|  2|    {x -> {y -> []}}|\n",
      "|  3|    {x -> {y -> []}}|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = {\n",
    "    'a':'a',\n",
    "    'x':{'y':['1']}\n",
    "}\n",
    "\n",
    "sche = build2(sch)\n",
    "\n",
    "print(sche)\n",
    "\n",
    "x = [[1,{'x':{'y':[1,2,3]}}],[2,{'x':{'y':[]}}],[3,{'x':{'y':[]}}]]\n",
    "s = spark.createDataFrame(x)\n",
    "s.printSchema()\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+----+-------+\n",
      "|    Name|Roll|Dept|Plan|Details|\n",
      "+--------+----+----+----+-------+\n",
      "|Sarbajit|  10| CSE|  A1|      1|\n",
      "|Shalmoli|  11|  CE|  A1|      2|\n",
      "|Sarbajit|  10| CSE|  B1|      1|\n",
      "|Shalmoli|  11|  CE|  A1|      3|\n",
      "|Shalmoli|  11|  CE|  A2|      4|\n",
      "|Shalmoli|  11|  CE|  B1|      5|\n",
      "|Sarbajit|  10|null|  B2|      3|\n",
      "|Shalmoli|  11|  CE|  A5|      6|\n",
      "+--------+----+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('sep',',').option('header','true').load(\"/home/sarbajit/Documents/Programs/spark/Data/test.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 8, 'Roll': 8, 'Dept': 7, 'Plan': 8, 'Details': 8}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.take(1)[0].asDict()\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "c= ['Name','Roll','Dept',\"Plan\",\"Details\"]\n",
    "df.select(*[ count(each).alias(each) for each in c]).take(1)[0].asDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+----+-------+---------+\n",
      "|    Name|Roll|Dept|Plan|Details|row_count|\n",
      "+--------+----+----+----+-------+---------+\n",
      "|Shalmoli|  11|  CE|  A1|      2|        2|\n",
      "|Shalmoli|  11|  CE|  A1|      3|        2|\n",
      "|Shalmoli|  11|  CE|  B1|      5|        1|\n",
      "|Shalmoli|  11|  CE|  A2|      4|        1|\n",
      "|Sarbajit|  10| CSE|  B2|      3|        1|\n",
      "|Shalmoli|  11|  CE|  A5|      6|        1|\n",
      "|Sarbajit|  10| CSE|  A1|      1|        1|\n",
      "|Sarbajit|  10| CSE|  B1|      1|        1|\n",
      "+--------+----+----+----+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window, functions as func\n",
    "\n",
    "windowSpec = Window.partitionBy(['Dept','Plan'])\n",
    "df = df.withColumn('row_count', func.count('Plan').over(windowSpec))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+\n",
      "| id|id2|  b|  c|  x|  y|\n",
      "+---+---+---+---+---+---+\n",
      "|  B|  1|  5|  3| YZ| Y1|\n",
      "|  C|  3| 10| -3| ZX| Z1|\n",
      "|  A|  1|  2|  3| XY| X1|\n",
      "+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([['A',1,2,3],['B',1,5,3],['C',3,10,-3]], schema=['id','id2','b','c'])\n",
    "df2 = spark.createDataFrame([['A',\"XY\"],['B','YZ'],['C','ZX']], schema=['id2','x'])\n",
    "df3 = spark.createDataFrame([['A',\"X1\"],['B','Y1'],['C','Z1']], schema=['id2','y'])\n",
    "\n",
    "df1 = df1.join(df2, how=\"left\", on=[df1['id']==df2['id2']]).drop(df2[\"id2\"])\n",
    "df1.join(df3, how=\"left\", on=[df1['id']==df3['id2']]).drop(df3[\"id2\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19913/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7acf4c38bcf2513b8cb734d877e501929c7062eca960ed16fb7ee116ea8ae2d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
